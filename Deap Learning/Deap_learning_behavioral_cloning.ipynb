{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6pC8PaXzq2js"
      },
      "source": [
        "# **MECS6616 Spring 2024 - Project 2**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "inY7y5CRo97q"
      },
      "source": [
        "# **Introduction**\n",
        "\n",
        "***IMPORTANT:***\n",
        "- **Before starting, make sure to read the [Assignment Instructions](https://courseworks2.columbia.edu/courses/197115/pages/assignment-instructions) page on Courseworks to understand the workflow and submission requirements for this project.**\n",
        "\n",
        "This project aims to demonstrate how neural networks can be used in a robotics setting. We will continue using the 2D maze environment introduced in Project 1 and learn to navigate an agent to a goal. Since neural networks can be more powerful models than the ones we had access to in Project 1, we can afford to make some changes to the 2D maze environment and make the problem more difficult. The project is divided into three parts: In Part I, you will train a simple Deep Neural Network (DNN) to predict the optimal action towards the goal given the agent position and the goal position. In Parts II and III, you will train Convolutional Neural Networks (CNNs) to predict the optimal action given images of the maze environment.\n",
        "\n",
        "<div>\n",
        "<img src=\"https://github.com/roamlab/robot-learning-S2023/blob/main/project2/imgs/P1_side.png?raw=true\" width=\"300\"/>\n",
        "</div>\n",
        "\n",
        "The figure above illustrates the simulation world, where the \"robot\" (also referred to as \"agent\") is represented by a green dot, and the goal location is marked by a red square. The agent's objective is to navigate to this goal location, avoiding any obstacles (depicted as black boxes) along the way.\n",
        "\n",
        "**Unlike the previous project, the robot and the goal are spawned at random positions in the maze.** Also, the action space now contains all four directions: 'up', 'down', 'left' and 'right'. Another change is that, in addition to the obstacle map shown above, we introduce two new obstacle maps as shown below. However, these new maps will not be used until Part III.\n",
        "\n",
        "<div>\n",
        "<img src=\"https://github.com/roamlab/robot-learning-S2023/blob/main/project2/imgs/map1.png?raw=true\" width=\"300\"/>\n",
        "<img src=\"https://github.com/roamlab/robot-learning-S2023/blob/main/project2/imgs/map2.png?raw=true\" width=\"300\"/>\n",
        "<img src=\"https://github.com/roamlab/robot-learning-S2023/blob/main/project2/imgs/map3.png?raw=true\" width=\"300\"/>\n",
        "</div>\n",
        "\n",
        "We want to learn to navigate the agent by imitating demonstrations from an expert user. In all three parts, you will be using data collected by a human controlling the agent via a keyboard for training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D1vjDH2fL9Tu"
      },
      "source": [
        "# **Project Setup (do NOT change)**\n",
        "\n",
        "***IMPORTANT:***\n",
        "- Do NOT change this \"*Project Setup*\" section\n",
        "- Do NOT install any other dependencies or a different version of an already provided package. You may, however, import other packages\n",
        "- Your code should go under the subsequent sections with headings \"*Part 1*\", \"*Part 2*\", and \"*Part 3*\"\n",
        "- You may find it useful to minimize sections using the arrows located to the left of each section heading\n",
        "\n",
        "You will be accessing data files located in a Google Drive folder. The following cell downloads the data from the cloud"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DptpWqf9awi7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8715102-bb8f-43de-ba2b-b2737fa5b8c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-03-20 03:24:21--  https://www.dropbox.com/scl/fi/slf93ry48wroksempi6or/project2.zip?rlkey=pw9m14sm98ga1259crst3ss5l\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.2.18, 2620:100:6028:18::a27d:4712\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.2.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://ucf5a3cd93c4db788c87077a2ef6.dl.dropboxusercontent.com/cd/0/inline/CPZqbfWfmr_07pTEbB4gpfb8IUPQPqIcFPK_2qtKr2YXONQUHPL63bPSh2nkzG2-Kq0tdTNgju0LQFBPRXMDVdAHn-tvSUwoWyrh2BgKfz_ewa0RSEkadR28XrO5xjOz7bYzywdjsd0_Ob8sFSpvV5bg/file# [following]\n",
            "--2024-03-20 03:24:21--  https://ucf5a3cd93c4db788c87077a2ef6.dl.dropboxusercontent.com/cd/0/inline/CPZqbfWfmr_07pTEbB4gpfb8IUPQPqIcFPK_2qtKr2YXONQUHPL63bPSh2nkzG2-Kq0tdTNgju0LQFBPRXMDVdAHn-tvSUwoWyrh2BgKfz_ewa0RSEkadR28XrO5xjOz7bYzywdjsd0_Ob8sFSpvV5bg/file\n",
            "Resolving ucf5a3cd93c4db788c87077a2ef6.dl.dropboxusercontent.com (ucf5a3cd93c4db788c87077a2ef6.dl.dropboxusercontent.com)... 162.125.65.15, 2620:100:6017:15::a27d:20f\n",
            "Connecting to ucf5a3cd93c4db788c87077a2ef6.dl.dropboxusercontent.com (ucf5a3cd93c4db788c87077a2ef6.dl.dropboxusercontent.com)|162.125.65.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: /cd/0/inline2/CPYtjR6xZAygV-v1ZViRry86g-f93h6O8Djgn2KXy3vKMN9HRIbOigocuAm2MT5hqyjEUlTvt9q3Acca8qLvmJUn5nzh47xPPdbBuxUpiRTUzmg4zXPjtBslZdcmjD9bp7lPczbXNZnZoZATWlXCLVbFwtMHj6AFFwGV2YUI6BdSpMDw136un_TTUkKVdwIDvJSWdEydAuTCtxzxXW7zg6u-h0gX_-9RZ-UjPhgZEI2ICHC_4ikCBgCwg8w7wpkuk5D4iHs0MnAH-qq8TrLJW9N9dYnETRMpxp93-ghMfKQr80CtbKbyjmz516jlxJ1gCqilwOhk0io_dwDxuR_rCbRsrEFtLyxE3p1C0OaqsxekbLqXkPjmgJFPYlq-URAh6yk/file [following]\n",
            "--2024-03-20 03:24:22--  https://ucf5a3cd93c4db788c87077a2ef6.dl.dropboxusercontent.com/cd/0/inline2/CPYtjR6xZAygV-v1ZViRry86g-f93h6O8Djgn2KXy3vKMN9HRIbOigocuAm2MT5hqyjEUlTvt9q3Acca8qLvmJUn5nzh47xPPdbBuxUpiRTUzmg4zXPjtBslZdcmjD9bp7lPczbXNZnZoZATWlXCLVbFwtMHj6AFFwGV2YUI6BdSpMDw136un_TTUkKVdwIDvJSWdEydAuTCtxzxXW7zg6u-h0gX_-9RZ-UjPhgZEI2ICHC_4ikCBgCwg8w7wpkuk5D4iHs0MnAH-qq8TrLJW9N9dYnETRMpxp93-ghMfKQr80CtbKbyjmz516jlxJ1gCqilwOhk0io_dwDxuR_rCbRsrEFtLyxE3p1C0OaqsxekbLqXkPjmgJFPYlq-URAh6yk/file\n",
            "Reusing existing connection to ucf5a3cd93c4db788c87077a2ef6.dl.dropboxusercontent.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 13621926 (13M) [application/zip]\n",
            "Saving to: ‘project2.zip?rlkey=pw9m14sm98ga1259crst3ss5l’\n",
            "\n",
            "project2.zip?rlkey= 100%[===================>]  12.99M  8.44MB/s    in 1.5s    \n",
            "\n",
            "2024-03-20 03:24:24 (8.44 MB/s) - ‘project2.zip?rlkey=pw9m14sm98ga1259crst3ss5l’ saved [13621926/13621926]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# DO NOT CHANGE\n",
        "# Download data\n",
        "!wget https://www.dropbox.com/scl/fi/slf93ry48wroksempi6or/project2.zip?rlkey=pw9m14sm98ga1259crst3ss5l&dl=0\n",
        "!mv project2.zip?rlkey=pw9m14sm98ga1259crst3ss5l project2.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8IYctlC5-gfU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "682c9210-68d0-4646-8514-04fe04338f11"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  project2.zip\n",
            "   creating: /content/project2/data/\n",
            "  inflating: /content/project2/data/all_maps.pkl  \n",
            "  inflating: /content/project2/data/map1.pkl  \n",
            "  inflating: /content/project2/data_utils.py  \n",
            "  inflating: /content/project2/dnn.py  \n",
            "   creating: /content/project2/imgs/\n",
            "  inflating: /content/project2/imgs/map1.png  \n",
            "  inflating: /content/project2/imgs/map2.png  \n",
            "  inflating: /content/project2/imgs/map3.png  \n",
            "  inflating: /content/project2/imgs/P1_side.png  \n",
            "   creating: /content/project2/mjcf/\n",
            "   creating: /content/project2/mjcf/common/\n",
            "  inflating: /content/project2/mjcf/common/materials.xml  \n",
            "  inflating: /content/project2/mjcf/common/skybox.xml  \n",
            "  inflating: /content/project2/mjcf/common/visual.xml  \n",
            "  inflating: /content/project2/mjcf/point_mass.xml  \n",
            "  inflating: /content/project2/mjcf/test_mjcf.xml  \n",
            "  inflating: /content/project2/score_policy.py  \n",
            "  inflating: /content/project2/simple_maze.py  \n"
          ]
        }
      ],
      "source": [
        "# Make sure you have successfully uploaded the zip file to Colab before running the line below.\n",
        "# If wget fails to pull the zip file, you can download the zipfile from dropbox and manually upload it to collab instead\n",
        "# If you do decide to manually upload the file, use the dropbox link in the previous cell (after wget) to access the file\n",
        "# Make sure the zip file is named \"project2.zip\", rename it before uploading (if necessary)\n",
        "# Upload the entire zip file to google colab. Do not unzip before uploading\n",
        "\n",
        "# Unzip the uploaded zip file\n",
        "!unzip project2.zip -d /content/\n",
        "!mv project2/* /content/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y2w_kfLOMFIG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "969a1509-1b6f-471f-97fd-113e41d63f79"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pybullet\n",
            "  Downloading pybullet-3.2.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (103.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.2/103.2 MB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting numpngw\n",
            "  Downloading numpngw-0.1.3-py3-none-any.whl (21 kB)\n",
            "Requirement already satisfied: numpy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from numpngw) (1.25.2)\n",
            "Installing collected packages: pybullet, numpngw\n",
            "Successfully installed numpngw-0.1.3 pybullet-3.2.6\n"
          ]
        }
      ],
      "source": [
        "# DO NOT CHANGE\n",
        "\n",
        "# Install required packages\n",
        "!pip install pybullet numpngw"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wz0ALBOu6coY"
      },
      "source": [
        "# Part I. Behavioral cloning with low dimensional data\n",
        "\n",
        "This part is a natural extension of Part II in Project 1, where your agent needs to learn a policy using labeled examples from an expert.\n",
        "\n",
        "Each labeled example $i$ will contain a tuple of the form $(o, a)^i$, where $o$ represents an observation and $a$ represents the action taken by the expert given that observation. You must simply learn to imitate the expert, a process also known as behavioral cloning. Note that while the observation space will be different in each part, the action space is the same for the rest of the project.\n",
        "\n",
        "We will be training a DNN policy to predict an action to be taken ('up', 'down', 'left', and 'right') based on the observation. **In Part I, the observation will contain the agent position and the current goal position.** (Since the goal is sampled randomly, the policy has to know the current goal to be reached). The environment thus returns an observation array of size (4, ) where the agent position is contained in the first two axes and the current goal position is contained in the next two. **In Part I, the map that the robot is navigating is always the same.**\n",
        "\n",
        "PyTorch and Tensorflow are two popular frameworks for building and training neural networks but for this class, we will be exclusively using PyTorch and you are allowed to use any of its features. A good starting point can be found [here](https://github.com/roamlab/robot-learning-S2024/blob/main/dnn_example.py).\n",
        "\n",
        "You will implement a class that inherits from `RobotPolicy` by providing implementations for the abstract methods from the class. These abstract methods will be re-used by future parts of the project, so do not edit them.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NAb3tE7h9uUd"
      },
      "outputs": [],
      "source": [
        "# DO NOT CHANGE\n",
        "# base class\n",
        "\n",
        "import abc\n",
        "\n",
        "\n",
        "class RobotPolicy(abc.ABC):\n",
        "\n",
        "    @abc.abstractmethod\n",
        "    def train(self, data):\n",
        "        \"\"\"\n",
        "            Abstract method for training a policy.\n",
        "\n",
        "            Args:\n",
        "                data: a dict that contains X (key = 'obs') and y (key = 'actions').\n",
        "\n",
        "                X is either rgb image (N, 64, 64, 3) OR  agent & goal pos (N, 4)\n",
        "\n",
        "            Returns:\n",
        "                This method does not return anything. It will just need to update the\n",
        "                property of a RobotPolicy instance.\n",
        "        \"\"\"\n",
        "\n",
        "    @abc.abstractmethod\n",
        "    def get_action(self, obs):\n",
        "        \"\"\"\n",
        "            Abstract method for getting action. You can do data preprocessing and feed\n",
        "            forward of your trained model here.\n",
        "            Args:\n",
        "                obs: an observation (64 x 64 x 3) rgb image OR (4, ) positions\n",
        "\n",
        "            Returns:\n",
        "                action: an integer between 0 to 3\n",
        "        \"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZH35w1j-Y7t3"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import torch\n",
        "torch.manual_seed(0)\n",
        "\n",
        "class POSBCRobot(RobotPolicy):\n",
        "    torch.manual_seed(0)\n",
        "    class SimpleNN(nn.Module):\n",
        "        def __init__(self):\n",
        "            super(POSBCRobot.SimpleNN, self).__init__()\n",
        "            self.fc1 = nn.Linear(4, 64)  # Input layer\n",
        "            self.fc2 = nn.Linear(64, 64) # Hidden layer\n",
        "            self.fc3 = nn.Linear(64, 64) # Hidden layer\n",
        "            self.fc4 = nn.Linear(64, 4)   # Output layer\n",
        "\n",
        "        def forward(self, x):\n",
        "            x = torch.relu(self.fc1(x))\n",
        "            x = torch.relu(self.fc2(x))\n",
        "            x = torch.relu(self.fc3(x))\n",
        "            return self.fc4(x)\n",
        "\n",
        "    def __init__(self):\n",
        "        self.model = self.SimpleNN()\n",
        "\n",
        "    def train(self, data):\n",
        "        # Convert input data to Tensors\n",
        "        X = torch.tensor(data['obs'], dtype=torch.float32)\n",
        "        y = torch.tensor(data['actions'], dtype=torch.long)\n",
        "\n",
        "        # Create DataLoader\n",
        "        dataset = TensorDataset(X, y)\n",
        "        dataloader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "        # Initialize the network, loss function, and optimizer\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        optimizer = optim.Adam(self.model.parameters(), lr=0.01)\n",
        "\n",
        "        # Training loop\n",
        "        for epoch in range(100):  # number of epochs\n",
        "            for inputs, labels in dataloader:\n",
        "                optimizer.zero_grad()\n",
        "                outputs = self.model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "    def get_action(self, obs):\n",
        "        self.model.eval()  # Set the model to evaluation mode\n",
        "        with torch.no_grad():\n",
        "            obs = torch.tensor(obs, dtype=torch.float32).unsqueeze(0)  # Add batch dimension\n",
        "            outputs = self.model(obs)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            return predicted.item()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JLchnZGWZYeP"
      },
      "source": [
        "## Evaluation and Grading\n",
        "\n",
        "We will evaluate your model by simply having the agent follow the commands that it provides.  We will evaluate for 100 different randomly sampled starting positions and goals. For each goal, we roll out the trained policy for 50 steps. After the 50 steps, we will evaluate the closest distance to the goal the agent has ended up. If the agent reaches < 0.1 distance from the goal, the episode is ended before 50 steps and the minimum distance will be recorded as 0. The score is the fraction of the initial distance to goal covered by the agent averaged over 100 trials. Your final grade will be computed based on this score.\n",
        "\n",
        "We will calculate the score using the formula :\n",
        "\n",
        "```score = avg[(init_dist -  min_dist) / init_dist]```\n",
        "\n",
        "We will auto-generate your grades using the code below. The grading of each part is separate from each other so you can get the grade right after each part is finished.\n",
        "\n",
        "The total points of this assignment are 15. According to the difficulty level of each part, parts 1, 2, and 3 have 4, 5, 6 points respectively.\n",
        "\n",
        "- Part 1: if your score >= 0.99, you will receive 4 / 4. Otherwise, your final grade will be score / 0.99 * 4.\n",
        "- Part 2: if your score >= 0.95, you will receive 5 / 5. Otherwise, your final grade will be score / 0.95 * 5.\n",
        "- Part 3: if your score >= 0.95, you will receive 6 / 6. Otherwise, your final grade will be score / 0.95 * 6.\n",
        "\n",
        "The score function for each part provides two extra arguments to assist your debugging.\n",
        "\n",
        "- gui: If this is set to True, you will save the behavior of the agents during evaluation as an animation file. This animation file can be visualized using the provided code below to help you understand the behavior of the agent. **Please set it to False before your submission as it will slow down evaluation.**\n",
        "- model: If you provide a path to a saved model, the score function will not train from scratch but will instead load the save model. **Please set it to None before submission.** Any models you generate during runtime will be automatically deleted when disconnected. The grader will train the model from scratch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZporTBmpmahZ"
      },
      "outputs": [],
      "source": [
        "# DO NOT CHANGE\n",
        "# Set up grading\n",
        "\n",
        "import score_policy\n",
        "import importlib\n",
        "importlib.reload(score_policy)\n",
        "from IPython.display import Image\n",
        "\n",
        "\n",
        "part1_bound = 0.99\n",
        "part2_bound = 0.95\n",
        "part3_bound = 0.95"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-c-Ob4uUlRM7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0d9a3e1-b659-44cc-b26b-aba387dc4f85"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "---\n",
            "Part 1 Score: 1.0\n",
            "Part 1 Grade: 1.00 / 0.99 * 4 = 4.00\n"
          ]
        }
      ],
      "source": [
        "# DO NOT CHANGE\n",
        "# Getting the score and grade for Part 1\n",
        "\n",
        "score1 = score_policy.score_pos_bc(policy=POSBCRobot(), gui=False, model=None)\n",
        "grade1 = score1 / part1_bound * 4 if score1 < part1_bound else 4\n",
        "\n",
        "print('\\n---')\n",
        "print(f'Part 1 Score: {score1}')\n",
        "print(f'Part 1 Grade: {score1:.2f} / {part1_bound:.2f} * 4 = {grade1:.2f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dx3AkUMamm4L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33f59fba-dd1b-477a-9ab6-cd87db1b1531"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ],
      "source": [
        "# Optionally, uncomment and run the code below if you have saved an animation (gui = True) that you want to visualize.\n",
        "\n",
        "# Image(filename='part_1_anim.png', width=200, height=200)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3s-mYCwfZql_"
      },
      "source": [
        "# Part II. Behavioral cloning with visual observations\n",
        "\n",
        "In this part, you are asked to do a similar task as Part I, **but the observations will be RGB image observations of the world**, similar to the ones you used to do localization in Part III of Project 1. To process the RGB images, you will be implementing a CNN using PyTorch. [The official PyTorch tutorial](https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html) is a good starting point. As in Part I, the map that the robot is navigating is always the same. **This means that your model really only has to learn how to figure out where the robot and the goal are located, and how to navigate around a fixed set of obstacles.**\n",
        "\n",
        "All requirements from your code, as well as the evaluation method, are unchanged compared to Part I. The only difference is the nature of the observation that is provided to you."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y9VmDjXiagTn"
      },
      "outputs": [],
      "source": [
        "# Implement your solution for Part 2 below\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import torch\n",
        "\n",
        "torch.manual_seed(0)\n",
        "\n",
        "\n",
        "# implement CNN class more clear to use\n",
        "class SimpleCNN(nn.Module):\n",
        "    torch.manual_seed(0)\n",
        "    def __init__(self):\n",
        "        super(SimpleCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 16, kernel_size=5, stride=1, padding=2)  # input channel 3 (RGB),output 16\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)  # pooling layer\n",
        "        self.conv2 = nn.Conv2d(16, 32, kernel_size=5, stride=1, padding=2)\n",
        "        self.dropout = nn.Dropout(0.5)  # Dropout rate 0.5\n",
        "        self.fc1 = nn.Linear(32 * 16 * 16, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 4)  # 4 action\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(torch.relu(self.conv1(x)))\n",
        "        x = self.pool(torch.relu(self.conv2(x)))\n",
        "        x = x.reshape(-1, 32 * 16 * 16)  # reshape needed\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        return self.fc3(x)\n",
        "\n",
        "\n",
        "class RGBBCRobot1(RobotPolicy):\n",
        "    torch.manual_seed(0)\n",
        "    def __init__(self):\n",
        "        super(RGBBCRobot1, self).__init__()\n",
        "        self.model = SimpleCNN()\n",
        "        self.optimizer = optim.Adam(self.model.parameters(), lr=0.001)\n",
        "        self.criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    def train(self, data):\n",
        "        # Tensor\n",
        "        X = torch.tensor(data['obs'], dtype=torch.float32)\n",
        "        y = torch.tensor(data['actions'], dtype=torch.long)\n",
        "\n",
        "        # make sure (N, H, W, C)\n",
        "        X = X.permute(0, 3, 1, 2)\n",
        "\n",
        "\n",
        "        dataset = TensorDataset(X, y)\n",
        "        dataloader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "        # trainning process\n",
        "        for epoch in range(15):\n",
        "            total_loss = 0.0\n",
        "            for inputs, labels in dataloader:\n",
        "                self.optimizer.zero_grad()\n",
        "                outputs = self.model(inputs)\n",
        "                loss = self.criterion(outputs, labels)\n",
        "                loss.backward()\n",
        "                self.optimizer.step()\n",
        "\n",
        "                total_loss += loss.item()  # accumulate loss\n",
        "\n",
        "            avg_loss = total_loss / len(dataloader)\n",
        "            #print(f\"Epoch {epoch+1}, Average Loss: {avg_loss:.4f}\") # debug purpose\n",
        "\n",
        "    def get_action(self, obs):\n",
        "        self.model.eval()\n",
        "        with torch.no_grad():\n",
        "            obs = torch.tensor(obs, dtype=torch.float32).unsqueeze(0)\n",
        "            obs = obs.permute(0, 3, 1, 2)  # [N, C, H, W]\n",
        "            outputs = self.model(obs)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            return predicted.item()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cj5Xje--lkiZ"
      },
      "source": [
        "## Evaluation and Grading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y2kSdH99oESr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e55298b-89e9-46a4-d218-6914d0520972"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "---\n",
            "Part 2 Score: 0.9053956137703933\n",
            "Part 2 Grade: 0.91 / 0.95 * 5 = 4.77\n"
          ]
        }
      ],
      "source": [
        "# DO NOT CHANGE\n",
        "# Getting the score and grade for Part 2\n",
        "\n",
        "score2 = score_policy.score_rgb_bc1(policy=RGBBCRobot1(), gui=False, model=None)\n",
        "grade2 = score2 / part2_bound * 5 if score2 < part2_bound else 5\n",
        "\n",
        "print('\\n---')\n",
        "print(f'Part 2 Score: {score2}')\n",
        "print(f'Part 2 Grade: {score2:.2f} / {part2_bound:.2f} * 5 = {grade2:.2f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TauW8ur-mhOV"
      },
      "outputs": [],
      "source": [
        "# Optionally, uncomment and run the code below if you have saved an animation (gui = True) that you want to visualize.\n",
        "\n",
        "# Image(filename='part_2_anim.png', width=200, height=200)0.9491843050708616"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1I6v1TgCctRp"
      },
      "source": [
        "# Part III. Behavioral cloning with visual observations - multiple maps\n",
        "\n",
        "This part is the same as  Part II except that it is trained and tested differently. **The training set involves expert demonstrations for the two new obstacle maps. And while testing, for each trial, a different obstacle map is randomly selected.** This means that your model has to learn how to reason about what an obstacle is, and how to go around it, based on nothing more than an image. The main objective of this part is to show that, when using a CNN, it is possible for a model to achieve this. The evaluation method for this part is the same as Part I and II."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AxQwN2MAdO09"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import torch\n",
        "\n",
        "torch.manual_seed(0)\n",
        "\n",
        "\n",
        "\n",
        "class SimpleCNN(nn.Module):\n",
        "# same with part two\n",
        "    torch.manual_seed(0)\n",
        "    def __init__(self):\n",
        "        super(SimpleCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 16, kernel_size=5, stride=1, padding=2)  # input 3 (RGB), output 16\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)  # pooling\n",
        "        self.conv2 = nn.Conv2d(16, 32, kernel_size=5, stride=1, padding=2)\n",
        "        # New convolutional layer\n",
        "        self.conv3 = nn.Conv2d(32, 64, kernel_size=5, stride=1, padding=2)  # input 32, output 64\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "        self.fc1 = nn.Linear(64 * 8 * 8, 120)  # Adjusted size\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 4)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(torch.relu(self.conv1(x)))\n",
        "        x = self.pool(torch.relu(self.conv2(x)))\n",
        "        x = self.pool(torch.relu(self.conv3(x)))  # Add pooling after conv3\n",
        "        x = x.reshape(-1, 64 * 8 * 8)  # Adjusted reshape for the new layer dimensions\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        return self.fc3(x)\n",
        "\n",
        "class RGBBCRobot2(RobotPolicy):\n",
        "    torch.manual_seed(0)\n",
        "    def __init__(self):\n",
        "        super(RGBBCRobot2, self).__init__()\n",
        "        self.model = SimpleCNN()\n",
        "        self.optimizer = optim.Adam(self.model.parameters(), lr=0.0005)\n",
        "        self.criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    def train(self, data):\n",
        "\n",
        "        X = torch.tensor(data['obs'], dtype=torch.float32)\n",
        "        y = torch.tensor(data['actions'], dtype=torch.long)\n",
        "\n",
        "\n",
        "        X = X.permute(0, 3, 1, 2)\n",
        "\n",
        "\n",
        "        dataset = TensorDataset(X, y)\n",
        "        dataloader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "\n",
        "        for epoch in range(8):\n",
        "            total_loss = 0.0\n",
        "            for inputs, labels in dataloader:\n",
        "                self.optimizer.zero_grad()\n",
        "                outputs = self.model(inputs)\n",
        "                loss = self.criterion(outputs, labels)\n",
        "                loss.backward()\n",
        "                self.optimizer.step()\n",
        "\n",
        "                total_loss += loss.item()\n",
        "\n",
        "            avg_loss = total_loss / len(dataloader)\n",
        "            #print(f\"Epoch {epoch+1}, Average Loss: {avg_loss:.4f}\")\n",
        "\n",
        "    def get_action(self, obs):\n",
        "        self.model.eval()\n",
        "        with torch.no_grad():\n",
        "            obs = torch.tensor(obs, dtype=torch.float32).unsqueeze(0)\n",
        "            obs = obs.permute(0, 3, 1, 2)\n",
        "            outputs = self.model(obs)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            return predicted.item()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aAky_Vu9l_EC"
      },
      "source": [
        "## Evaluation and Grading\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PM4edyLqpT3v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d75c179-d190-440b-c583-d3b0c0120b0b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "---\n",
            "Part 3 Score: 0.9321780342669144\n",
            "Part 3 Grade: 0.93 / 0.95 * 6 = 5.89\n"
          ]
        }
      ],
      "source": [
        "# DO NOT CHANGE\n",
        "# Getting the score and grade for Part 3\n",
        "\n",
        "score3 = score_policy.score_rgb_bc2(policy=RGBBCRobot2(), gui=False, model=None)\n",
        "grade3 = score3 / part3_bound * 6 if score3 < part3_bound else 6\n",
        "\n",
        "print('\\n---')\n",
        "print(f'Part 3 Score: {score3}')\n",
        "print(f'Part 3 Grade: {score3:.2f} / {part3_bound:.2f} * 6 = {grade3:.2f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UmyAWRQwB2Rr"
      },
      "outputs": [],
      "source": [
        "# Optionally, uncomment and run the code below if you have saved an animation (gui = True) that you want to visualize.\n",
        "\n",
        "# Image(filename='part_3_anim.png', width=200, height=200)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bNJs1PcRpnSw"
      },
      "source": [
        "# Other Requirements and Hints\n",
        "\n",
        "- **Training time**: To keep auto-grading feasible, your total training time must be strictly under 3 mins, 15mins, and 10 mins for parts 1, 2, and 3. These time budgets are more than enough to achieve full credits on this project. Note that longer training time does not necessarily mean higher performance because of overfitting. The faster your network trains the better!\n",
        "- **Memory usage**: Make sure your code does not require too much memory. The required amount of RAM for this assignment should not be more than 8GB.\n",
        "- **NO GPU**: No GPU is required or allowed for this assignment.\n",
        "- **Reproducibility**: We have ensured that the randomness of the environment is deterministic. To get reproducible scores you must ensure your model training and prediction are also reproducible. The randomly initialized weights of the neural network should be made repeatable using seeding. You can add PyTorch seeding method below and see [PyTorch Reproducibility](https://pytorch.org/docs/stable/notes/randomness.html) to learn more.\n",
        "  ```\n",
        "  import torch\n",
        "  torch.manual_seed(0)\n",
        "  ```\n",
        "- **Classifier**: In all the parts we are training a neural network to solve a classification problem and it is important to use a reasonable loss function. For example, the MSE (mean squared classification) error has drawbacks related to sensitivity. Cross entropy loss usually has good performance for classification tasks and you can find the documentation for it [here](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss) and is further explained below. However, note that you are free to use any loss function you like.\n",
        "  - Cross entropy is a concept from information theory which is defined for two probability distributions. Cross entropy is minimum when the two distributions involved are the same and this is the property that makes it useful as a loss function in the context of machine learning. The idea is to minimize the cross entropy between the prediction distribution and the label distribution. For our case where we are training a neural network for classification, we can have the network output a score for each action. Cross entropy can be computed from these scores by converting to probability values (using softmax) and comparing it with the label distribution. The label distribution is obtained simply by assigning a probability of 1 to ground truth action and 0 to all other actions. Once trained, the best action can found by just choosing the action with the highest probability (i.e., the highest score) as predicted by the network.\n",
        "- **Optimizer**: While it is possible to use a simple optimizer to achieve the desired accuracy, the training time can be quite high. There exist a number of optimizers implemented in PyTorch that have much faster convergence.\n",
        "- **Parameter tuning**: Keep your architectures simple and slowly add complexity (more layers/kernels) to improve accuracy. Remember \"To Err is Human\" and the expert data (collected by a human) that you are training on is not perfect. Having a 100% training accuracy (very small training loss) might not be the best for achieving the highest score. So make sure your model does not overfit during training.\n",
        "- **PyTorch input shape**: Notice that the expected input shape to CONV2D in PyTorch is (N, C, H, W), where N is the batch size, C is the number of channels, H is the image height and W is the image width. You will need to switch axes for the incoming images in order for them to be correctly passed to the first convolution layer."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}